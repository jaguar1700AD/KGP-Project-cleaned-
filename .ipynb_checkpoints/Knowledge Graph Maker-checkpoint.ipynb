{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dir = 'D:\\\\KGP\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_words = set()\n",
    "with open(global_dir + 'zip_words.txt','r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for word in csv_reader:\n",
    "        zip_words.add(word[0])\n",
    "zip_len = len(zip_words)\n",
    "zip_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set(set_string):\n",
    "    if set_string == 'set()':\n",
    "        return set()\n",
    "    else:\n",
    "        return ast.literal_eval(set_string)\n",
    "    \n",
    "def is_title(name):\n",
    "    if len(name) >= 3 and name[-3] == '(' and name[-2].isdigit() and name[-1] == ')':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def valid_entity(name):\n",
    "    if is_title(name) and (len(name) == 3 or name[-4] in string.punctuation):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_index = {}\n",
    "\n",
    "length = 0\n",
    "with open(global_dir + 'Extractions\\\\ent_index.txt','r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "        for entity in csv_reader:\n",
    "            ent_index[entity['name']] = int(entity['index'])\n",
    "            length = max([length, int(entity['index'])])\n",
    "length += 1 \n",
    "            \n",
    "ent_name = [0] * length\n",
    "with open(global_dir + 'Extractions\\\\ent_index.txt','r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "        for entity in csv_reader:\n",
    "            ent_name[int(entity['index'])] = entity['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [0] * length\n",
    "with open(global_dir + 'Extractions\\\\index_merged_extractions.txt','r') as csv_file:\n",
    "\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "\n",
    "        for entity in csv_reader:\n",
    "            _index = int(entity['index'])\n",
    "            entities[_index] = {}\n",
    "            entities[_index]['type'] = get_set(entity['type'])\n",
    "            entities[_index]['context_dict'] = ast.literal_eval(entity['context_dict'])\n",
    "            entities[_index]['co_occ'] = get_set(entity['co_occ'])\n",
    "            #entities[entity['name']]['zipf'] = get_set(entity['zipf_vector'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a Manpage title entity -> entity\n",
    "# One way relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section : 9 Progress: 408 / 408 3 1 \r"
     ]
    }
   ],
   "source": [
    "file_name = '1a.txt'\n",
    "with open(global_dir + 'Extractions\\\\Graph\\\\' + file_name, 'w') as file_int:\n",
    "    fieldnames = ['ent1','ent2']\n",
    "    csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for section in range(1, 10):\n",
    "\n",
    "        directory = global_dir + 'Extractions\\\\man' + str(section) + '\\\\'\n",
    "        anc_directory = global_dir + 'Extractions\\\\Anchors\\\\man' + str(section) + '\\\\'\n",
    "        all_files = os.listdir(directory)\n",
    "        n = 1\n",
    "\n",
    "        for file in all_files:\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"Section : %d Progress: %d / %d \\r\" % (section, n, len(all_files)))\n",
    "            \n",
    "            title_set = set()\n",
    "            with open(anc_directory + file,'r') as csv_file:\n",
    "                csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "                for line in csv_reader:\n",
    "                    title_set.add(line['title'])\n",
    "\n",
    "            with open(directory + file,'r') as csv_file:\n",
    "                csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "                for entity in csv_reader:\n",
    "                    name = entity['name']\n",
    "                    if name not in title_set:\n",
    "                        for title in title_set:\n",
    "                            new_dict = {'ent1' : ent_index[title], 'ent2' : ent_index[name]}\n",
    "                            csv_writer.writerow(new_dict)\n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b03401f7e6f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'Extractions\\\\Graph\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcsv_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mtitle_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ent1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\csv.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mlf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check\n",
    "file_name = '1a.txt'\n",
    "title_set = set()\n",
    "with open(global_dir + 'Extractions\\\\Graph\\\\' + file_name,'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "    for line in csv_reader:\n",
    "        title_set.add(line['ent1'])\n",
    "        \n",
    "for title in title_set:\n",
    "    print(ent_name[int(title)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b +- k window\n",
    "# One way relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '1b.txt'\n",
    "with open(global_dir + 'Extractions\\\\Graph\\\\' + file_name, 'w') as file_int:\n",
    "    fieldnames = ['ent1','ent2']\n",
    "    csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "    csv_writer.writeheader()\n",
    "    for index1 in range(length):\n",
    "        for index2 in entities[index1]['co_occ']:\n",
    "            new_dict = {'ent1' : index1, 'ent2' : index2}\n",
    "            csv_writer.writerow(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Anchor link\n",
    "# One way relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section : 9 Progress: 408 / 408 3 1 \r"
     ]
    }
   ],
   "source": [
    "bad = 0\n",
    "\n",
    "file_name = '2.txt'\n",
    "with open(global_dir + '\\\\Extractions\\\\Graph\\\\' + file_name, 'w') as file_int:\n",
    "    fieldnames = ['ent1','ent2']\n",
    "    csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for section in range(1, 10):\n",
    "\n",
    "        directory = global_dir + 'Extractions\\\\Anchors\\\\man' + str(section) + '\\\\'\n",
    "        all_files = os.listdir(directory)\n",
    "        n = 1\n",
    "\n",
    "        for file in all_files:\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"Section : %d Progress: %d / %d \\r\" % (section, n, len(all_files)))\n",
    "\n",
    "            with open(directory + file,'r') as csv_file:\n",
    "                csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "                for line in csv_reader:\n",
    "                    for link in get_set(line['links']):\n",
    "                        new_dict = {'ent1' : ent_index[line['title']], 'ent2' : ent_index[link]}\n",
    "                        csv_writer.writerow(new_dict)\n",
    "\n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36986"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(global_dir + 'Extractions\\\\Anchors\\\\man1\\\\'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Similar context_dict link from holling\n",
    "# Two way relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_list = [0] * length\n",
    "\n",
    "for i in range(length):\n",
    "    cont_list[i] = entities[i]['context_dict']\n",
    "        \n",
    "num = len(cont_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(30)\n",
    "random.shuffle(cont_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate occurence_count for all entities\n",
    "occ_count = {}\n",
    "for index in range(length):\n",
    "    entity = entities[index]\n",
    "    occ_count[index] = 0\n",
    "    for context in entity['context_dict']:\n",
    "        occ_count[index] += entity['context_dict'][context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4230 / 4230 \r"
     ]
    }
   ],
   "source": [
    "file_name = '3.txt'\n",
    "with open(global_dir + 'Extractions\\\\Graph\\\\' + file_name, 'a', newline = '') as file_int:\n",
    "    \n",
    "    fieldnames = ['ent1','ent2']\n",
    "    csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "    csv_writer.writeheader()\n",
    "    \n",
    "    threshold = 0.9 # 2 entities will be connected by an edge if no of common contexts > threshold * no of contexts for both entities\n",
    "    lim1 = 0\n",
    "    lim2 = num\n",
    "    for i in range(lim1, lim2):\n",
    "\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"%d / %d \\r\" % (i - lim1 + 1, lim2 - lim1))\n",
    "\n",
    "        cont1 = cont_list[i]\n",
    "\n",
    "        for j in range(i + 1, num):\n",
    "\n",
    "            cont2 = cont_list[j]\n",
    "\n",
    "            n1 = 0 # No of common contexts of entity1\n",
    "            n2 = 0 # No of common contexts of entity2\n",
    "\n",
    "            for context in cont1:\n",
    "                if context in cont2:\n",
    "                    n1 += cont1[context]\n",
    "                    n2 += cont2[context]\n",
    "\n",
    "\n",
    "            if (n1 >= threshold * occ_count[i]) and (n2 >= threshold * occ_count[j]):\n",
    "                new_dict = {'ent1' : i, 'ent2' : j}\n",
    "                csv_writer.writerow(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 zipf based relation\n",
    "# Two way relation\n",
    "\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_ent_set = set()\n",
    "four_index = {}\n",
    "\n",
    "max_ent_len = 0\n",
    "for name in ent_name:\n",
    "    cand = name.replace(' ', '')\n",
    "    four_ent_set.add(cand)\n",
    "    four_index[cand] = ent_index[name]\n",
    "    max_ent_len = max([max_ent_len, len(nltk.word_tokenize(name))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find entity pairs between +- window_size\n",
    "# word_list is list of [entity_index, start_word_index, entity_length]\n",
    "# Remove pairs between the same words in the end\n",
    "\n",
    "def find_pairs(word_list):\n",
    "    n = len(word_list)\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        start_limit_up = word_list[i][1] + word_list[i][2] + window_size\n",
    "        start_limit_down = word_list[i][1] + word_list[i][2]\n",
    "        j = i + 1\n",
    "        while j < n and word_list[j][1] <= start_limit_up and word_list[j][1] >= start_limit_down:\n",
    "            pairs.append([i, j])\n",
    "            j += 1\n",
    "    \n",
    "    for ind, pair in enumerate(pairs):\n",
    "        if word_list[pair[0]][0] == word_list[pair[1]][0]:\n",
    "            pairs[ind] = None\n",
    "            \n",
    "    return pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section : 1 Progress: 670 / 37656 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8cf2c28a6db1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Section : %d Progress: %d / %d \\r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msauce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m         \u001b[1;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_lxml.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParserError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleSaxData\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxData\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_lxml.py\u001b[0m in \u001b[0;36mdata\u001b[1;34m(self, content)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessing_instruction_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_name = '4.txt'\n",
    "with open(global_dir + '\\\\Extractions\\\\Graph\\\\' + file_name, 'w') as file_int:\n",
    "    fieldnames = ['ent1','relation', 'ent2']\n",
    "    csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for section in range(1, 10):\n",
    "        directory = global_dir + 'man' + str(section) + '\\\\'\n",
    "        all_files = os.listdir(directory)\n",
    "        n = 1\n",
    "        for file in all_files:\n",
    "            \n",
    "            with open(directory + file, 'rb') as sauce:\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stdout.write(\"Section : %d Progress: %d / %d \\r\" % (section, n, len(all_files)))\n",
    "\n",
    "                soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "\n",
    "                sentences = nltk.sent_tokenize(soup.get_text())\n",
    "\n",
    "                for sentence in sentences:\n",
    "\n",
    "                    word_list = []\n",
    "                    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "                    for start in range(len(words)):\n",
    "                        for length in range(1, min(len(words) - start, max_ent_len) + 1):\n",
    "\n",
    "                            candidate = ''.join(words[start: start + length])\n",
    "\n",
    "                            if candidate in four_ent_set:\n",
    "                                word_list.append([four_index[candidate], start, length])\n",
    "\n",
    "                            if is_title(candidate):\n",
    "                                cand = candidate[:-3]\n",
    "                                if cand in four_ent_set:\n",
    "                                    word_list.append([four_index[cand], start, length])\n",
    "                            else:\n",
    "                                cand = candidate + '(' + str(section) + ')'\n",
    "                                if cand in four_ent_set:\n",
    "                                    word_list.append([four_index[cand], start, length])\n",
    "                    \n",
    "                    pairs = find_pairs(word_list)\n",
    "                    \n",
    "                    for pair in pairs:\n",
    "                        \n",
    "                        if pair == None:\n",
    "                            continue\n",
    "                        \n",
    "                        list0 = word_list[pair[0]]\n",
    "                        list1 = word_list[pair[1]]\n",
    "                        \n",
    "                        start_ind = list0[1] + list0[2]\n",
    "                        end_in = list1[1]\n",
    "                        relation = set(words[start_ind : end_in]) & zip_words\n",
    "                        \n",
    "                        try:\n",
    "                            #new_dict = {'file' : words, 'ent1' : ent_name[list0[0]], 'relation' : relation, 'ent2' : ent_name[list1[0]]}\n",
    "                            new_dict = {'ent1' : list0[0], 'relation' : relation, 'ent2' : list1[0]}\n",
    "                            csv_writer.writerow(new_dict)\n",
    "                        except:\n",
    "                            pass\n",
    "                n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
